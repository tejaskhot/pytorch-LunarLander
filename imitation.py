import sys
import argparse
import numpy as np
import keras
import random
import gym
import pdb
from keras.utils import to_categorical

class Imitation():
    def __init__(self, model_config_path, expert_weights_path):
        # Load the expert model.
        with open(model_config_path, 'r') as f:
            self.expert = keras.models.model_from_json(f.read())
        self.expert.load_weights(expert_weights_path)
        
        # Initialize the cloned model (to be trained).
        with open(model_config_path, 'r') as f:
            self.model = keras.models.model_from_json(f.read())
        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    def run_expert(self, env, render=False):
        # Generates an episode by running the expert policy on the given env.
        return Imitation.generate_episode(self.expert, env, render)
        
    def run_model(self, env, render=False):
        # Generates an episode by running the cloned policy on the given env.
        return Imitation.generate_episode(self.model, env, render)

    @staticmethod
    def generate_episode(model, env, render=False):
        # Generates an episode by running the given model on the given env.
        # Returns:
        # - a list of states, indexed by time step
        # - a list of actions, indexed by time step
        # - a list of rewards, indexed by time step

        states = []
        actions = []
        rewards = []
        state = env.reset()
        steps = 0
        while True:
            states.append(state)
            action = np.argmax( model.predict(np.array([state])))            
            actions.append(action)
            state, reward, is_terminal, _ = env.step(action)
            rewards.append(reward)
            steps +=1
            if render:
                env.render()
            if is_terminal:
                break
        return states, actions, rewards
    
    def train(self, env, num_episodes=100, num_epochs=50, render=False):
        # Trains the model on training data generated by the expert policy.
        # Args:
        # - env: The environment to run the expert policy on. 
        # - num_episodes: # episodes to be generated by the expert.
        # - num_epochs: # epochs to train on the data generated by the expert.
        # - render: Whether to render the environment.
        # Returns the final loss and accuracy.        
        
        loss = 0
        acc = 0
        x_train = np.empty((0,env.observation_space.shape[0]))
        y_train = np.empty((0))
        for iter_episode in range(num_episodes):
            states,actions,rewards = self.run_expert(env=env,render=render)	
            x_train = np.append(x_train,np.asarray(states),axis=0)
            y_train = np.append(y_train,np.asarray(actions),axis=0)
        y_train_cat = to_categorical(y_train, num_classes=env.action_space.n)
        metric = self.model.fit(x_train, y_train_cat, epochs=num_epochs, shuffle=True, verbose=0)
        loss = metric.history['loss']
        acc = metric.history['acc']
        return loss[-1], acc[-1]

def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-config-path', dest='model_config_path',
                        type=str, default='./models/LunarLander-v2-config.json',
                        help="Path to the model config file.")
    parser.add_argument('--expert-weights-path', dest='expert_weights_path',
                        type=str, default='./models/LunarLander-v2-weights.h5',
                        help="Path to the expert weights file.")

    # https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse
    parser_group = parser.add_mutually_exclusive_group(required=False)
    parser_group.add_argument('--render', dest='render',
                              action='store_true',
                              help="Whether to render the environment.")
    parser_group.add_argument('--no-render', dest='render',
                              action='store_false',
                              help="Whether to render the environment.")
    parser.set_defaults(render=False)
    return parser.parse_args()

def main(args):
    # Parse command-line arguments.
    args = parse_arguments()
    model_config_path = args.model_config_path
    expert_weights_path = args.expert_weights_path
    render = args.render
    
    # Create the environment.
    env = gym.make('LunarLander-v2')
    agent = Imitation(model_config_path, expert_weights_path)

    #Running the expert model
    print('\n################Running the expert################\n')
    episode_rewards_expert = []
    for _ in range(50):
        states, actions, rewards = agent.run_expert(env, render)
        total_reward = np.sum(rewards)
        episode_rewards_expert.append(total_reward)
    print('Mean of rewards of expert : ' + str(np.mean(episode_rewards_expert)))
    print('SD of rewards of expert   : ' + str(np.std(episode_rewards_expert)))

    #Training and running our model
    print('\n\n#################Training and running the model#################\n')
    dataset_size = [1,10,50,100]
    for num_episodes in dataset_size:
        #Training the model
        agent = Imitation(model_config_path, expert_weights_path)
        loss, acc = agent.train(env=env, num_episodes=num_episodes, num_epochs=50, render=render)
        print('Traning size : ' + str(num_episodes) + ' Training accuracy : ' + str(acc))
        #Running the model
        episode_rewards_model = []
        for _ in range(50):
            states, actions, rewards = agent.run_model(env, render)
            total_reward = np.sum(rewards)
            episode_rewards_model.append(total_reward)
        print('Traning size : ' + str(num_episodes) + ' Mean of rewards   : ' + str(np.mean(episode_rewards_model)))
        print('Traning size : ' + str(num_episodes) + ' SD of rewards     : ' + str( np.std(episode_rewards_model)))
        print('\n')
		
if __name__ == '__main__':
  main(sys.argv)
